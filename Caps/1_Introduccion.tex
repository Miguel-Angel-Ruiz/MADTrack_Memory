\chapter{Introduction}
\label{cap:Introduction}

With the passage of time, the development of \acrfull{AI} has become of increasing interest due to the powerful tools it can provide to any
organisation \cite{AIRise}. Since the development of \emph{The Bombe}, the machine that was able to decode the \emph{Enigma} machine in 1939,
passing through a whole set of ups and downs and even a silent winter before its comeback in 2015 thanks to Deep Learning, Artificial Intelligence
is being applied in a number of fields, sometimes even reaching the point of having the potential of threatening human safety and raising awareness
of the need of regulations for its use.

The development of these models has been made possible thanks to the contribution of a number of organisations, such as Google, Microsoft and OpenAI,
which invested an objectively significant amount of time and money (reaching a total investment of 24.0 billion dollars in 2018 \cite{AIRise}) to develop systems as
broad as the GPT-4 model, which is able to generate text from both imaged and textual inputs \cite{GPT4}, and is capable of helping professionals in solving
doubts in a large number of fields.

With time, the development of these models became increasingly complex and difficult, and this would be reflected in the number of iterations required for
a model to reach the desired level of accuracy in its predictions. Moreover, the application of these techniques on areas where little data was available
made this development even harder. Some companies even started to employ their own resources to gather new data to train their models, which led to a model
having more iterations as the available data grew. Summarizing, a model (and even a dataset) can become complex structures that have many evolutionary
stages within their lifecycle.

Many organisations, aware of the increasing complexity of the lifecycle of datasets and models, saw a market opportunity to develop systems that would manage
these lifecycles in an organized and efficient way, producing the AI and dataset configuration management tools, such as MLFlow \cite{MLflow}, DVC, which were
open source and available to everyone, as well as premium tools such as Neptune.ai, which are \acrfull{PaaS} that provide even further monitoring
capabilities.

The aim of this Final Degree Project is to tackle this issue in a specific particular case. In the coming subsections, the reasons that motivated the elaboration
of this project, along with the challenges it faces and how they will be solved, will be thoroughly described. Finally, an overview of the structure of the document
will be given, so as to give readers the necessary information to follow the development documentation of the project.

\section{Motivation}

It is of common knowledge that Artificial Intelligence has gained significant importance in the 2020s. The development of tools such as the GPT models
have headed a revolution in the way humans solve both simple and complex tasks. A revolution that was made possible with the intervention of multiple
companies and organisations, and a considerable amount of money and time invested in the development of these models \cite{AIRise}.

UBOTICA Technologies is a pioneer company that develops \acrshort{AI} \acrfull{CV} solutions. This means that, as a company, they use Artificial Intelligence
techniques to extract information from images. These solutions are then integrated in embedded systems with limited capabilities, and that are part of a 
bigger, more complex system. The domains where these solutions are used are mainly in the space industry with their new CogniSAT-6 project \cite{UBOTICACS6}, 
and recently even in the culinary industry. The development of the various solutions of the company requires multiple iterations where the models are trained,
validated and tested, either in existing datasets, or in new ones. Moreover, the CogniSAT-6 system has the capability for creating new datasets out of self-taken
images, which may be periodically added to the existing datasets, or even used to replace the existing ones.

This continuous rise of the available models and datasets, paired with a lack of a real control protocol over the new and improved versions of an AI model or dataset,
has often led to situations where an abnormal amount of time is spent searching for the desired dataset, and accessing the necessary training configuration
that produced an specific result.

The aim of MADTrack is to develop a system that establishes a robust configuration management basis for these datasets and AI models. A system that can integrate
both new and existing datasets and models in a distributed, remote environment, and that will make the best use of the available resources the company dedicates
to this management.

\section{Problems and Solutions}

The main problem to be tackled in this Final Degree Project is the lack of a robust configuration management protocol over the produced datasets and models. another
problem is the difficulty at not only determining the correct configuration, but also bringing it to the environment where it is going to be used, and the need to adapt the
system to the end users' existing methods to ensure familiarity.

Furthermore, another problem resides within the protocols and deployment requirements of the system, which may need special authentication protocols from
the environment where the system is going to be deployed, as well as from the environment where the models and datasets are stored (which in turn may also differ
from the one where the system is running). All of this may result in the need to develop a distributed system, where many components interact with each other
to satisfy a complex need. Moreover, the system must be easy to integrate in bigger workflows, so the study of the available pipelines the company uses and the 
possible integration points of the system within them may be taken as another problem.

Finally, the last problem is the limitations of the company to dedicate resources for the storage of all the data produced by the model and dataset lifecycles.
The developed system must, hence, make an efficient use of the resources available, so as to maximize the amount of data that could be stored and managed. Also,
the system must be able to adapt to the needs of a growing company, where many requests may be taken concurrently.

For the resolution of all these three problems, the system will be formed by a server that will satisfy requests and perform the necessary storage and
fetching operations to bring both the datasets and the models to the users, whilst the client will be a library that enables the integration of the
necessary items into the configuration management, sending the evolutionary changes of a dataset or model to the server that stores them.


%\begin{table}[H]%
%	\centering
%	\caption{Usos ilícitos de la IA en el TFG}
%	\label{tab:ia}
%	\begin{tabular}{ | p{0.3\linewidth} | p{0.3\linewidth} | p{0.3\linewidth} |}
%		\hline
%		\textbf{Uso de la IA} & \textbf{Descripción} & \textbf{Riesgo} \\
%		\hline
%		Generación completa o parcial de textos para la memoria.&
%		Presentar como propio un texto obtenido casi en su totalidad por una IA. &
%		\textbf{Plagio académico}: el autor no es quien firma el texto.\\
%		\hline
%		Evitar el trabajo intelectual o de análisis. &
%		Usar IA para hacer razonamientos, interpretaciones o críticas sin comprensión real. &
%		Viola los principios de \textbf{evaluación auténtica} y aprendizaje significativo. \\
%		\hline
%		Falsificación de datos. & Generar datos simulados o inexistentes para experimentos, encuestas o estadísticas. &
%		Constituye \textbf{fraude académico}.\\
%		\hline
%		Traducción automática sin revisión. &
%		Entregar traducciones automáticas sin control de calidad. &
%		Puede derivar en \textbf{errores conceptuales}.\\
%		\hline
%	\end{tabular}
%\end{table}


\section{Document Structure}

According to the steps necessary to fully describe the elaboration process of this Final Degree Project, the following structure has been decided:

\begin{enumerate}

\item \textbf{Introduction}. The domain and main problematics are described, as well as what solutions the project will establish to these problematics.

\item \textbf{Objective}. The main objective, as well as the specific objectives of the project, are enumerated and detailed.

\item \textbf{State-of-the-art research}. This chapter contains the results of an extensive research and study of all the concepts and technologies relevant to the development of the project.

\item \textbf{Methodology}. Description of the development framework and methodology to be used, its specific application to the development of the project and the equipment used during it.

\item \textbf{Desarrollo y resultados}. En este capítulo se explica cómo se han llevado a cabo las fases del trabajo cumpliendo el plan previsto y enumerando los resultados obtenidos.

\item \textbf{Conclusions}. A summary of the achieved results will be made, as well as the proposal of the future work to be carried out on this project.

\item \textbf{Bibliografía}. Lista de las referencias bibliográficas que se hayan citado en el texto. Recuerda que no debes incluir fuentes de información relacionada que no hayas citado explícitamente en la memoria.

\item \textbf{Anexos}. Contenidos auxiliares que complementan del trabajo, como manuales de uso, diagramas, figuras, tablas, listados de código, etcétera.
\end{enumerate}









